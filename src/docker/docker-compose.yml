# File: src\docker\docker-compose.yml

services:
  # Node Exporter pour Prometheus
  node_exporter:
    image: prom/node-exporter:v1.6.0
    container_name: node-exporter
    ports:
      - "9100:9100"
    networks:
      - monitoring

  # Prometheus pour la collecte des métriques
  prometheus:
    image: prom/prometheus:v2.40.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ../../src/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - node_exporter
      - fastapi
    networks:
      - monitoring

  # Grafana pour le dashboard des métriques
  grafana:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - monitoring

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-satisfaction
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: "-Xms2g -Xmx2g"
    ports:
      - "9200:9200"
    volumes:
      - es-data:/usr/share/elasticsearch/data
    networks:
      - monitoring

  # Kibana : dashboard pour Elasticsearch
  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.0
    container_name: kibana-satisfaction
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    ports:
      - "5601:5601"
    volumes:
      - ../../src/kibana_exports:/opt/kibana_exports:ro
    networks:
      - monitoring

  # API FastAPI qui interagit avec Elasticsearch
  fastapi:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_api
    container_name: fastapi-satisfaction
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_HOST: "http://elasticsearch:9200"
    ports:
      - "8000:8000"
    volumes:
      - ../../src:/app/src
    networks:
      - monitoring

  # Frontend Streamlit
  streamlit:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_frontend
    container_name: streamlit-satisfaction
    depends_on:
      - fastapi
    ports:
      - "8501:8501"
    volumes:
      - ../../src:/app/src
      - ../../src/frontend/assets:/app/assets
    networks:
      - monitoring

  # Postgres pour Airflow
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - monitoring

  # Airflow init (création DB et user admin)
  airflow-init:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_airflow
    container_name: airflow-init
    user: "50000:0"  # UID pour Airflow
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor                # Exécution locale
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"                 # Ne pas charger les DAGs d’exemple
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"   # Activer les DAGs dès la création
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - airflow-data:/opt/airflow
      - ../../src/airflow/dags:/opt/airflow/dags
      - ../../src/etl:/opt/airflow/etl
      - ../../src/etl/data:/opt/airflow/etl/data
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
      airflow dags unpause etl_reviews_batch
      "
    networks:
      - monitoring

  # Airflow Webserver
  airflow-webserver:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_airflow
    container_name: airflow-webserver
    user: "50000:0"
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__HOST: 0.0.0.0
    volumes:
      - airflow-data:/opt/airflow
      - ../../src/airflow/dags:/opt/airflow/dags
      - ../../src/etl:/opt/airflow/etl
      - ../../src/etl/data:/opt/airflow/etl/data
    ports:
      - "8081:8080"
    command: webserver
    restart: unless-stopped
    networks:
      - monitoring

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: ../../
      dockerfile: src/docker/Dockerfile_airflow
    container_name: airflow-scheduler
    user: "50000:0"
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - airflow-data:/opt/airflow
      - ../../src/airflow/dags:/opt/airflow/dags
      - ../../src/etl:/opt/airflow/etl
      - ../../src/etl/data:/opt/airflow/etl/data
    command: >
      bash -c "
      airflow dags unpause etl_reviews_batch &&
      airflow scheduler
      "
    restart: unless-stopped
    networks:
      - monitoring

volumes:
  es-data:        # Elasticsearch data
  grafana-data:   # Grafana data
  airflow-data:   # Airflow data
  pgdata:         # Postgres data

networks:
  monitoring:
    driver: bridge
